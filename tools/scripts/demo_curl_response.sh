#!/bin/bash

echo "ğŸ§ª LLMServingSim curlå“åº”æ¼”ç¤º"
echo "================================"
echo ""

echo "ğŸ“ å‘é€çš„è¯·æ±‚ï¼š"
echo "curl -X POST http://localhost:8000/v1/chat/completions \\"
echo "  -H \"Content-Type: application/json\" \\"
echo "  -d '{\"model\": \"meta-llama/Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}], \"max_tokens\": 100}'"
echo ""

echo "ğŸ“¤ é¢„æœŸçš„å“åº”æ ¼å¼ï¼š"
cat << 'EOF'
{
  "id": "chatcmpl-a1b2c3d4",
  "object": "chat.completion", 
  "created": 1703123456,
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "[LLMServingSim] Request queued for processing. Input tokens: 2, Max output tokens: 100"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 2,
    "completion_tokens": 100,
    "total_tokens": 102
  }
}
EOF

echo ""
echo "ğŸ” å…³é”®è¯´æ˜ï¼š"
echo "â€¢ contentå­—æ®µåŒ…å«ä»¿çœŸå“åº”ï¼Œä¸æ˜¯çœŸå®AIç”Ÿæˆçš„æ–‡æœ¬"
echo "â€¢ usageå­—æ®µæ˜¾ç¤ºtokenä½¿ç”¨ç»Ÿè®¡"
echo "â€¢ è¯·æ±‚ä¼šè¢«æ·»åŠ åˆ°LLMServingSimçš„å¤„ç†é˜Ÿåˆ—ä¸­"
echo "â€¢ ASTRA-Simä¼šè¿›è¡Œæ€§èƒ½ä»¿çœŸè®¡ç®—"
echo ""

echo "ğŸš€ è¦å®é™…æµ‹è¯•ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡å™¨ï¼š"
echo "./run_openai_server.sh"
echo ""
echo "ç„¶ååœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œä¸Šé¢çš„curlå‘½ä»¤"
