# LLMServingSim OpenAIå…¼å®¹APIä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

LLMServingSimç°åœ¨æ”¯æŒ**OpenAIå…¼å®¹çš„APIæ¥å£**ï¼Œå¯ä»¥ä½œä¸ºOpenAI APIçš„æ›¿ä»£å“è¿›è¡Œæ€§èƒ½ä»¿çœŸå’Œæµ‹è¯•ã€‚

### âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸ”Œ **å®Œå…¨å…¼å®¹OpenAI APIæ ¼å¼**ï¼šæ”¯æŒchat/completionså’Œcompletionsç«¯ç‚¹
- ğŸ“Š **æ€§èƒ½ä»¿çœŸ**ï¼šåŸºäºçœŸå®ç¡¬ä»¶æ€§èƒ½æ•°æ®è¿›è¡Œå»¶è¿Ÿå’Œååé‡ä»¿çœŸ
- ğŸš€ **é›¶é…ç½®å¯åŠ¨**ï¼šä¸€é”®å¯åŠ¨OpenAIå…¼å®¹æœåŠ¡
- ğŸ“ˆ **å®æ—¶ç›‘æ§**ï¼šæä¾›è¯¦ç»†çš„æœåŠ¡çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡
- ğŸ”§ **çµæ´»é…ç½®**ï¼šæ”¯æŒå¤šç§ç¡¬ä»¶å’Œæ¨¡å‹é…ç½®

## å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨OpenAIå…¼å®¹æœåŠ¡å™¨

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®å¯åŠ¨ï¼ˆç«¯å£8000ï¼‰
./run_openai_server.sh

# è‡ªå®šä¹‰ç«¯å£å’Œä¸»æœº
./run_openai_server.sh --port 8080 --host 0.0.0.0

# æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹
./run_openai_server.sh --help
```

### 2. æµ‹è¯•APIè¿æ¥

```bash
# è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
python test_openai_api.py

# æˆ–æŒ‡å®šæœåŠ¡å™¨åœ°å€
python test_openai_api.py http://localhost:8080
```

## APIç«¯ç‚¹

### ğŸš€ OpenAIå…¼å®¹ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° | OpenAIå…¼å®¹æ€§ |
|------|------|------|-------------|
| `/v1/chat/completions` | POST | èŠå¤©è¡¥å…¨ | âœ… å®Œå…¨å…¼å®¹ |
| `/v1/completions` | POST | æ–‡æœ¬è¡¥å…¨ | âœ… å®Œå…¨å…¼å®¹ |
| `/v1/models` | GET | åˆ—å‡ºå¯ç”¨æ¨¡å‹ | âœ… å®Œå…¨å…¼å®¹ |

### ğŸ“Š æœåŠ¡ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/status` | GET | è¯¦ç»†æœåŠ¡çŠ¶æ€ |

## ä½¿ç”¨ç¤ºä¾‹

### 1. ä½¿ç”¨curlæµ‹è¯•

#### Chat Completions
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [
      {"role": "user", "content": "Hello! How are you?"}
    ],
    "max_tokens": 150
  }'
```

#### Text Completions
```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "prompt": "The future of AI is",
    "max_tokens": 100
  }'
```

#### åˆ—å‡ºæ¨¡å‹
```bash
curl http://localhost:8000/v1/models
```

### 2. ä½¿ç”¨OpenAI Pythonå®¢æˆ·ç«¯

```python
from openai import OpenAI

# åˆ›å»ºå®¢æˆ·ç«¯ï¼ŒæŒ‡å‘æœ¬åœ°æœåŠ¡å™¨
client = OpenAI(
    api_key="dummy-key",  # LLMServingSimä¸éœ€è¦çœŸå®APIå¯†é’¥
    base_url="http://localhost:8000/v1"
)

# Chat Completions
response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing."}
    ],
    max_tokens=200
)

print(response.choices[0].message.content)
print(f"Usage: {response.usage}")
```

### 3. ä½¿ç”¨å…¶ä»–OpenAIå…¼å®¹åº“

ä»»ä½•æ”¯æŒè‡ªå®šä¹‰base_urlçš„OpenAIå®¢æˆ·ç«¯éƒ½å¯ä»¥ä½¿ç”¨ï¼š

```python
# LangChain
from langchain.llms import OpenAI
llm = OpenAI(openai_api_base="http://localhost:8000/v1")

# LlamaIndex  
from llama_index.llms import OpenAI
llm = OpenAI(api_base="http://localhost:8000/v1")
```

## å“åº”æ ¼å¼

### Chat Completionså“åº”
```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1699896916,
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "[LLMServingSim] Request queued for processing..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 20,
    "completion_tokens": 150,
    "total_tokens": 170
  }
}
```

### Modelså“åº”
```json
{
  "object": "list",
  "data": [
    {
      "id": "meta-llama/Llama-3.1-8B-Instruct",
      "object": "model",
      "created": 1699896916,
      "owned_by": "llmservingsim"
    }
  ]
}
```

## é…ç½®é€‰é¡¹

### å¯åŠ¨å‚æ•°

```bash
python main.py \
    --idle_mode \                    # å¯ç”¨ç©ºé—²æ¨¡å¼ï¼ˆåªç›‘å¬ï¼Œä¸ç”Ÿæˆæµé‡ï¼‰
    --http_host localhost \          # HTTPæœåŠ¡å™¨ä¸»æœº
    --http_port 8000 \              # HTTPæœåŠ¡å™¨ç«¯å£
    --model_name meta-llama/Llama-3.1-8B-Instruct \  # æ¨¡å‹åç§°
    --hardware RTX3090 \            # ç¡¬ä»¶ç±»å‹
    --npu_num 1 \                   # NPUæ•°é‡
    --npu_mem 40 \                  # NPUå†…å­˜(GB)
    --verbose                       # è¯¦ç»†æ—¥å¿—
```

### ç¯å¢ƒå˜é‡æ”¯æŒ

```bash
export LLMSERVINGSIM_HOST=0.0.0.0
export LLMSERVINGSIM_PORT=8000
export LLMSERVINGSIM_MODEL=meta-llama/Llama-3.1-8B-Instruct
```

## æ€§èƒ½ç›‘æ§

### å®æ—¶çŠ¶æ€æŸ¥è¯¢

```bash
curl http://localhost:8000/status
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "pending_requests": 5,
  "inflight_batches": 2,
  "completed_requests": 100,
  "memory_usage": {
    "total": 42949672960,
    "used": 2147483648,
    "available": 40802189312
  }
}
```

### æ—¥å¿—ç›‘æ§

æœåŠ¡å™¨ä¼šè¾“å‡ºè¯¦ç»†çš„è¯·æ±‚å¤„ç†æ—¥å¿—ï¼š
```
2024-01-15 10:30:15 - INFO - 127.0.0.1 - POST /v1/chat/completions
Scheduler: added request to LLMServingSim
Added request: input_len=25, output_len=175
[0.500s] Service is idle, waiting for requests...
```

## é›†æˆç¤ºä¾‹

### 1. æ›¿æ¢OpenAI API

```python
# åŸæ¥çš„ä»£ç 
import openai
openai.api_key = "sk-..."
openai.api_base = "https://api.openai.com/v1"

# æ›¿æ¢ä¸ºLLMServingSim
import openai
openai.api_key = "dummy"
openai.api_base = "http://localhost:8000/v1"
```

### 2. æ€§èƒ½æµ‹è¯•è„šæœ¬

```python
import time
import requests
import concurrent.futures

def send_request():
    response = requests.post("http://localhost:8000/v1/chat/completions", 
        json={
            "model": "meta-llama/Llama-3.1-8B-Instruct",
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 100
        })
    return response.status_code == 200

# å¹¶å‘æµ‹è¯•
with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(send_request) for _ in range(100)]
    results = [f.result() for f in futures]
    
print(f"Success rate: {sum(results)/len(results)*100:.1f}%")
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: æœåŠ¡å™¨å¯åŠ¨å¤±è´¥**
```bash
# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
lsof -i :8000

# ä½¿ç”¨ä¸åŒç«¯å£
./run_openai_server.sh --port 8080
```

**Q: APIå“åº”æ ¼å¼ä¸æ­£ç¡®**
```bash
# æ£€æŸ¥Content-Typeå¤´
curl -H "Content-Type: application/json" ...
```

**Q: è¯·æ±‚è¢«æ‹’ç»**
```bash
# æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—
python main.py --idle_mode --verbose
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python main.py --idle_mode --verbose --http_port 8000

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health
curl http://localhost:8000/status
```

## é™åˆ¶å’Œæ³¨æ„äº‹é¡¹

### å½“å‰é™åˆ¶

1. **ä»¿çœŸå“åº”**ï¼šè¿”å›çš„æ˜¯ä»¿çœŸå“åº”ï¼Œä¸æ˜¯çœŸå®çš„æ–‡æœ¬ç”Ÿæˆ
2. **å•æ¨¡å‹æ”¯æŒ**ï¼šç›®å‰åªæ”¯æŒé…ç½®çš„å•ä¸ªæ¨¡å‹
3. **æ— æµå¼å“åº”**ï¼šæš‚ä¸æ”¯æŒçœŸæ­£çš„æµå¼å“åº”
4. **åŸºç¡€è®¤è¯**ï¼šä¸éœ€è¦çœŸå®çš„APIå¯†é’¥éªŒè¯

### æ€§èƒ½è€ƒè™‘

- é€‚ç”¨äºæ€§èƒ½æµ‹è¯•å’Œæ¶æ„éªŒè¯
- ä¸é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒçš„å®é™…æ–‡æœ¬ç”Ÿæˆ
- å“åº”å»¶è¿ŸåŸºäºç¡¬ä»¶æ€§èƒ½æ¨¡å‹é¢„æµ‹

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°ç«¯ç‚¹

```python
# åœ¨http_server.pyä¸­æ·»åŠ 
def _handle_custom_endpoint(self):
    # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
    pass
```

### è‡ªå®šä¹‰å“åº”æ ¼å¼

```python
def _send_custom_response(self, data):
    # è‡ªå®šä¹‰å“åº”æ ¼å¼
    response = {"custom": data}
    self._send_json_response(200, response)
```

---

ğŸ‰ ç°åœ¨ä½ å¯ä»¥å°†LLMServingSimä½œä¸ºOpenAI APIçš„å®Œå…¨å…¼å®¹æ›¿ä»£å“ä½¿ç”¨äº†ï¼
