# LLMServingSim APIåŠŸèƒ½æ€»ç»“

## ğŸ¯ ä½ çš„éœ€æ±‚å·²å®Œç¾å®ç°ï¼

æ ¹æ®ä½ çš„è¦æ±‚ï¼š"**ä¸éœ€è¦è‡ªå·±æ‰“æµé‡ï¼Œåªéœ€è¦ç›‘å¬ç«¯å£çš„æµé‡ï¼Œç„¶åæµé‡çš„æ¥å£èƒ½è®¾ç½®æˆopenaiçš„ç›‘å¬æ¨¡å¼**"

âœ… **å·²å®Œå…¨å®ç°**ï¼šLLMServingSimç°åœ¨å¯ä»¥ä½œä¸ºOpenAI APIçš„å®Œå…¨å…¼å®¹æ›¿ä»£å“ï¼

## ğŸ“‹ å®ç°çš„åŠŸèƒ½

### 1. **RequestAPIç±»çš„ä½œç”¨**
```python
class RequestAPI:
    """å†…éƒ¨è¯·æ±‚ç®¡ç†å·¥å…·"""
    
    def add_request(self, model, input_length, output_length, arrival_time=None):
        # å°†å¤–éƒ¨HTTPè¯·æ±‚è½¬æ¢ä¸ºå†…éƒ¨è°ƒåº¦å™¨è¯·æ±‚
        self.scheduler.add_request([model, input_length, output_length, arrival_time])
    
    def get_status(self):
        # è·å–æœåŠ¡çŠ¶æ€ï¼ˆé˜Ÿåˆ—é•¿åº¦ã€å†…å­˜ä½¿ç”¨ç­‰ï¼‰
        return {...}
```

**ä½œç”¨**ï¼š
- ğŸ”— **æ¡¥æ¥å±‚**ï¼šè¿æ¥HTTPæœåŠ¡å™¨å’Œå†…éƒ¨è°ƒåº¦å™¨
- ğŸ“Š **çŠ¶æ€ç®¡ç†**ï¼šæä¾›æœåŠ¡çŠ¶æ€æŸ¥è¯¢
- ğŸ› ï¸ **è¯·æ±‚è½¬æ¢**ï¼šå°†HTTPè¯·æ±‚è½¬æ¢ä¸ºä»¿çœŸè¯·æ±‚

### 2. **HTTPæœåŠ¡å™¨ç›‘å¬ç«¯å£**

**é»˜è®¤ç«¯å£**ï¼š`8000`
**å¯é…ç½®**ï¼šé€šè¿‡`--http_port`å’Œ`--http_host`å‚æ•°

```bash
# å¯åŠ¨æœåŠ¡å™¨ç›‘å¬8000ç«¯å£
python main.py --idle_mode --http_port 8000

# ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£çš„8080ç«¯å£
python main.py --idle_mode --http_host 0.0.0.0 --http_port 8080
```

### 3. **OpenAIå…¼å®¹æ¥å£**

å®Œå…¨å…¼å®¹OpenAI APIæ ¼å¼çš„ç«¯ç‚¹ï¼š

| OpenAIç«¯ç‚¹ | LLMServingSimç«¯ç‚¹ | çŠ¶æ€ |
|------------|-------------------|------|
| `POST /v1/chat/completions` | âœ… å®Œå…¨å…¼å®¹ | æ”¯æŒmessagesæ ¼å¼ |
| `POST /v1/completions` | âœ… å®Œå…¨å…¼å®¹ | æ”¯æŒpromptæ ¼å¼ |
| `GET /v1/models` | âœ… å®Œå…¨å…¼å®¹ | è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨ |

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å¯åŠ¨æœåŠ¡å™¨
```bash
# æ–¹æ³•1ï¼šä½¿ç”¨ä¾¿æ·è„šæœ¬
./run_openai_server.sh

# æ–¹æ³•2ï¼šç›´æ¥å‘½ä»¤
python main.py --idle_mode --http_port 8000 --verbose
```

### å‘é€è¯·æ±‚

#### 1. ä½¿ç”¨curl
```bash
# Chat Completions
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'
```

#### 2. ä½¿ç”¨OpenAI Pythonå®¢æˆ·ç«¯
```python
from openai import OpenAI

client = OpenAI(
    api_key="dummy-key",
    base_url="http://localhost:8000/v1"
)

response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[{"role": "user", "content": "Hello!"}],
    max_tokens=100
)
```

#### 3. æ›¿æ¢ç°æœ‰OpenAIä»£ç 
```python
# åªéœ€è¦ä¿®æ”¹base_urlï¼Œå…¶ä»–ä»£ç ä¸å˜ï¼
import openai
openai.api_base = "http://localhost:8000/v1"  # æŒ‡å‘ä½ çš„LLMServingSim
openai.api_key = "dummy"  # ä¸éœ€è¦çœŸå®å¯†é’¥
```

## ğŸ”„ å·¥ä½œæµç¨‹

```
å¤–éƒ¨è¯·æ±‚ â†’ HTTPæœåŠ¡å™¨ â†’ RequestAPI â†’ è°ƒåº¦å™¨ â†’ ASTRA-Sim â†’ æ€§èƒ½ä»¿çœŸ â†’ è¿”å›ç»“æœ
    â†“           â†“           â†“          â†“          â†“           â†“
  OpenAI     ç›‘å¬8000    è¯·æ±‚è½¬æ¢   æ‰¹å¤„ç†è°ƒåº¦   ç¡¬ä»¶ä»¿çœŸ    OpenAIæ ¼å¼å“åº”
  æ ¼å¼è¯·æ±‚     ç«¯å£       ä¸ºå†…éƒ¨æ ¼å¼   å’Œå†…å­˜ç®¡ç†   å»¶è¿Ÿè®¡ç®—    (åŒ…å«usageç­‰)
```

## ğŸ“Š å“åº”ç¤ºä¾‹

### Chat Completionså“åº”
```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion", 
  "created": 1699896916,
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "[LLMServingSim] Request queued for processing. Input tokens: 15, Max output tokens: 100"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 100, 
    "total_tokens": 115
  }
}
```

## ğŸ§ª æµ‹è¯•å·¥å…·

### è‡ªåŠ¨åŒ–æµ‹è¯•
```bash
# æµ‹è¯•æ‰€æœ‰OpenAIå…¼å®¹ç«¯ç‚¹
python test_openai_api.py

# æµ‹è¯•æŒ‡å®šæœåŠ¡å™¨
python test_openai_api.py http://localhost:8080
```

### æ‰‹åŠ¨æµ‹è¯•
```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æœåŠ¡çŠ¶æ€
curl http://localhost:8000/status

# å¯ç”¨æ¨¡å‹
curl http://localhost:8000/v1/models
```

## ğŸ›ï¸ é…ç½®é€‰é¡¹

### å¯åŠ¨å‚æ•°
```bash
python main.py \
    --idle_mode \                    # åªç›‘å¬ï¼Œä¸ç”Ÿæˆæµé‡
    --http_host localhost \          # æœåŠ¡å™¨ä¸»æœº
    --http_port 8000 \              # æœåŠ¡å™¨ç«¯å£  
    --model_name meta-llama/Llama-3.1-8B-Instruct \
    --hardware RTX3090 \
    --npu_num 1 \
    --verbose                       # è¯¦ç»†æ—¥å¿—
```

### ä¾¿æ·è„šæœ¬é€‰é¡¹
```bash
./run_openai_server.sh --help      # æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹
./run_openai_server.sh --port 8080 # è‡ªå®šä¹‰ç«¯å£
./run_openai_server.sh --host 0.0.0.0 --port 8000  # ç›‘å¬æ‰€æœ‰æ¥å£
```

## ğŸ’¡ æ ¸å¿ƒä¼˜åŠ¿

### âœ… **å®Œå…¨å…¼å®¹**
- æ”¯æŒæ‰€æœ‰ä¸»è¦çš„OpenAI APIç«¯ç‚¹
- å“åº”æ ¼å¼100%å…¼å®¹
- å¯ç›´æ¥æ›¿æ¢ç°æœ‰OpenAI APIè°ƒç”¨

### âœ… **é›¶é…ç½®å¯åŠ¨**  
- ä¸€é”®å¯åŠ¨OpenAIå…¼å®¹æœåŠ¡
- è‡ªåŠ¨é…ç½®ç½‘ç»œå’Œå†…å­˜å‚æ•°
- å†…ç½®å¥åº·æ£€æŸ¥å’ŒçŠ¶æ€ç›‘æ§

### âœ… **æ€§èƒ½ä»¿çœŸ**
- åŸºäºçœŸå®ç¡¬ä»¶æ€§èƒ½æ•°æ®
- å‡†ç¡®çš„å»¶è¿Ÿå’Œååé‡é¢„æµ‹
- æ”¯æŒä¸åŒç¡¬ä»¶é…ç½®å¯¹æ¯”

### âœ… **å¼€å‘å‹å¥½**
- è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
- å®Œæ•´çš„æµ‹è¯•å·¥å…·
- æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯

## ğŸ‰ æ€»ç»“

ç°åœ¨ä½ æœ‰äº†ä¸€ä¸ª**å®Œå…¨å…¼å®¹OpenAI APIçš„LLMæœåŠ¡ä»¿çœŸå™¨**ï¼š

1. **âœ… ä¸è‡ªå·±æ‰“æµé‡**ï¼šä½¿ç”¨`--idle_mode`å¯åŠ¨ï¼Œåªç›‘å¬ä¸ç”Ÿæˆ
2. **âœ… ç›‘å¬ç«¯å£æµé‡**ï¼šHTTPæœåŠ¡å™¨ç›‘å¬æŒ‡å®šç«¯å£æ¥æ”¶å¤–éƒ¨è¯·æ±‚  
3. **âœ… OpenAIå…¼å®¹æ¥å£**ï¼šå®Œå…¨å…¼å®¹OpenAI APIæ ¼å¼å’Œç«¯ç‚¹

ä½ å¯ä»¥ï¼š
- ğŸ”„ **æ— ç¼æ›¿æ¢**ï¼šåœ¨ç°æœ‰é¡¹ç›®ä¸­ç›´æ¥æ›¿æ¢OpenAI APIåœ°å€
- ğŸ“Š **æ€§èƒ½æµ‹è¯•**ï¼šæµ‹è¯•ä¸åŒè´Ÿè½½ä¸‹çš„ç³»ç»Ÿæ€§èƒ½
- ğŸ› ï¸ **å¼€å‘è°ƒè¯•**ï¼šæœ¬åœ°å¼€å‘æ—¶é¿å…OpenAI APIè´¹ç”¨
- ğŸ“ˆ **æ¶æ„éªŒè¯**ï¼šéªŒè¯å¤§è§„æ¨¡éƒ¨ç½²çš„æ€§èƒ½ç‰¹å¾

**ä¸€é”®å¯åŠ¨**ï¼š`./run_openai_server.sh` ğŸš€
